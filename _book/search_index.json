[
["homework.html", "Chapter 4 Homework 4.1 First Exam Period", " Chapter 4 Homework 4.1 First Exam Period 4.1.1 Assigned Problems NOTE: This homework submission is for both Ralph Trane and Alex Hayes. For almost all of the problems here, we worked together on a whiteboard. We then took pictures or handwritten notes of the solutions and wrote them up here. Exercise 4.1 (Ex 2) Let \\(\\C\\) be a collection of subsets of \\(\\Omega\\) and let \\(\\Gamma = \\{ \\F | \\F \\text{ is a } \\sigma \\text{-field on } \\Omega \\text{ and } \\C \\subset \\F \\}\\). Show that \\(\\Gamma \\neq \\emptyset\\) and \\(\\sigma(\\C) = \\cap_{\\F \\in \\Gamma} \\F\\). Solution (Ex 2). Let \\(\\P(\\Omega)\\) be the collection of all subsets of \\(\\Omega\\). We know that this is a \\(\\sigma\\)-field. It also contains \\(\\C\\). Hence, \\(\\Gamma \\neq \\emptyset\\). By definition, \\(\\sigma(\\C)\\) is the smallest \\(\\sigma\\)-field that contains \\(\\C\\), hence \\(\\sigma(\\C) \\in \\Gamma\\) and \\(\\sigma(\\C) \\subset \\F\\) for all \\(\\F \\in \\Gamma\\). Therefore, \\(\\sigma(\\C) \\subset \\cap_{\\F \\in \\Gamma} \\F\\). But since \\(\\sigma(\\C) \\in \\Gamma\\), \\(\\sigma(\\C) \\in \\cap_{\\F \\in \\Gamma} \\F\\), which in turn ensures that \\(\\cap_{\\F \\in \\Gamma} \\F \\subset \\sigma(\\C)\\). Hence \\(\\sigma(\\C) = \\cap_{\\F \\in \\Gamma} \\F\\). Exercise 4.2 (Ex 5) a) Let \\(\\C\\) be a \\(\\pi\\)-system and \\(\\mathcal{D}\\) be a \\(\\lambda\\)-system such that \\(\\C \\subset \\mathcal{D}\\). Show that \\(\\sigma(\\C) \\subset \\mathcal{D}\\). Solution (Ex 5). TODO Exercise 4.3 (Ex 12) Let \\(\\nu\\) and \\(\\lambda\\) be two measures on \\((\\Omega, \\F)\\) such that \\(\\nu(A) = \\lambda(A)\\) for any \\(A \\in \\C \\subset \\F\\), where \\(\\C\\) is a \\(\\pi\\)-system (??). Assume that \\(\\nu\\) is \\(\\sigma\\)-finite (??). Show that \\(\\nu(A) = \\lambda(A)\\) for all \\(A \\in \\sigma(\\C)\\). Solution (Ex 12). Let \\(\\F = \\{A \\in \\sigma(\\C) | \\nu(A) = \\lambda(A)\\}\\). Then \\(\\C \\subset \\F\\). If we can show that \\(\\F\\) is a \\(\\sigma\\)-field, then \\(\\sigma(\\C) \\subset \\F\\) (since \\(\\sigma(\\C)\\) is the smallest \\(\\sigma\\)-field that contains \\(\\C\\)), which proves that \\(\\nu(A) = \\lambda(A)\\) for all \\(A \\in \\sigma(\\C)\\). Exercise 4.4 (Ex 14) Prove proposition 1.4 (proposition ??) Solution (Ex 14 (i)). Assume \\(f\\) is Borel. Then \\(f^{-1}(A) \\in \\F\\) for all open sets \\(A \\in \\B\\), hence \\(f^{-1}(a, \\infty) \\in \\F\\). Now assume \\(f^{-1}(a, \\infty) \\in \\F\\) for all \\(a \\in \\R\\), and let \\(\\mathcal{G} = \\{ A \\in \\B | f^{-1}(A) \\in \\F \\}\\). So, \\((a, \\infty) \\in \\mathcal{G}\\) for all \\(a \\in \\R\\). If we can show that \\(\\mathcal{G}\\) is a \\(\\sigma\\)-field, then we will have that \\(\\sigma((a, \\infty)) = \\B \\subset \\mathcal{G}\\), hence \\(f^{-1}(B) \\in \\F\\) for all \\(B \\in \\B\\), meaning that \\(f\\) is measurable. So let us prove that \\(\\mathcal{G}\\) is a \\(\\sigma\\)-field. First of all, \\(f^{-1}(\\emptyset) = \\emptyset \\in \\F\\). Second, let \\(A \\in \\mathcal{G}\\). Since \\(f^{-1}(A^C) = (f^{-1}(A))^C \\in \\F\\) (\\(\\F\\) is a \\(\\sigma\\)-field and \\(f^{-1}(A) \\in \\F\\), so \\((f^{-1}(A))(^C) \\in \\F\\)). Finally, let \\(A_1, A_2, \\ldots\\) be a sequence of sets such that \\(A_i \\in \\mathcal{G}\\) for all \\(i\\). Then \\(f^{-1}\\left(\\cup_{i=1}^\\infty A_i \\right) = \\cup_{i=1}^\\infty f^{-1}(A_i)\\). Since \\(f^{-1}(A_i) \\in \\F\\) for all \\(i\\) and \\(\\F\\) is a \\(\\sigma\\)-field, \\(\\cup_{i=1}^\\infty f^{-1}(A_i) \\in \\F\\), so \\(\\cup_{i=1}^\\infty A_i \\in \\mathcal{G}\\). So \\(\\mathcal{G}\\) is a \\(\\sigma\\)-field, which concludes the proof. Solution (Ex 14 (ii)). Assume \\(f\\) and \\(g\\) are Borel functions. Let \\(a,b \\in \\R\\). \\(af\\) is Borel, since \\[ (af)^{-1}((c,\\infty)) = \\left\\{ \\omega \\in \\Omega : a\\cdot f(\\omega) \\in (c, \\infty) \\right\\}. \\] If \\(a \\neq 0\\), \\[\\begin{aligned} (af)^{-1}((c,\\infty)) &amp;= \\left\\{ \\omega \\in \\Omega : f(\\omega) \\in (\\tfrac{c}{a}, \\infty) \\right\\} \\\\ &amp;= f^{-1}(\\tfrac{c}{a}, \\infty). \\end{aligned}\\] Since \\(f\\) is Borel, this is a measurable set (by (i)). If \\(a = 0\\), then \\[(af)^{-1}((c,\\infty)) = \\left\\{ \\begin{matrix} \\Omega &amp; \\text{ if } c \\le 0 \\\\ \\emptyset &amp; \\text{ if } c &lt; 0 \\end{matrix} \\right . \\] In either case, \\((af)^{-1}((c, \\infty)) \\in \\F\\). Since it holds that for all \\(a,c \\in \\R\\) that \\((af)^{-1}((c,\\infty)) \\in \\F\\), \\(af\\) is measurable by (i). Let \\(c \\in \\R\\). Now consider the sum of \\(f\\) and \\(g\\): \\[\\begin{aligned} (f + g)^{-1}((c, \\infty)) &amp;= \\left\\{ \\omega \\in \\Omega : f(\\omega) + g(\\omega) &gt; c \\right\\} \\\\. &amp;= \\cup_{t \\in \\mb{Q}} \\{\\omega \\in \\Omega : f(\\omega) &gt; c - t \\} \\cap \\{\\omega \\in \\Omega : g(\\omega) &gt; t \\} \\\\ &amp;= \\cup_{t \\in \\mb{Q}} f^{-1}((c-t, \\infty)) \\cap g^{-1}((t, \\infty)). \\end{aligned}\\] Since \\(f\\) and \\(g\\) are both measurable, \\(f^{-1}((c-t, \\infty)) \\in \\F\\) and \\(g^{-1}((t, \\infty))\\in \\F\\) for all \\(t \\in \\R\\). Hence, the intersection of the two is measurable for any \\(t \\in \\R\\), which in turn implies that the union over all rational numbers is measurable (countable union of measurable sets). Hence, \\(f+g\\) is measurable. Combine the two results to get the final result.1 Solution (Ex 14 (iii)). TODO Solution (Ex 14 (iv)). Assume \\(f\\) is measurable from \\((\\Omega, \\F)\\) to \\((\\Lambda, \\mathcal{G})\\), and \\(g\\) measureable from \\((\\Lambda, \\mathcal{G})\\) to \\((\\Delta, \\mathcal{H})\\). Let \\(H \\in \\mathcal{H}\\). We want to show that \\((g \\circ f)^{-1}(H) \\in \\F\\), since this would mean \\(g \\circ f\\) is measurable. So, \\[\\begin{aligned} (g \\circ f)^{-1}(H) &amp;= \\left\\{ \\omega \\in \\Omega | g(f(\\omega)) \\in H \\right\\} \\\\ &amp;= \\left\\{ \\omega \\in \\Omega | f(\\omega) \\in g^{-1}(H) \\right\\} \\\\ &amp;= f^{-1}(g^{-1}(H)). \\end{aligned}\\] Since \\(g\\) is measurable, \\(g^{-1}(H) \\in \\mathcal{G}\\), and since \\(f\\) is measurable, \\(f^{-1}(g^{-1}(H)) \\in \\F\\). So, \\(g \\circ f\\) is measurable. Solution (Ex 14 (v)). Let \\(f: \\Omega \\to \\R^p\\), where \\(\\Omega\\) is a borel set. Assume \\(f\\) is continuous. Then, if \\(A\\) is an open set, \\(f^{-1}(A)\\) is an open set, and therefore borel. Hence, \\(f^{-1}((a,\\infty))\\) is a borel set for all \\(a\\), and by \\((i)\\) we have that \\(f\\) is a borel function. Exercise 4.5 (Ex 19) Let \\(\\{f_n\\}\\) be a sequence of Borel functions on a measurable space. Show that \\(\\sigma(f_1, f_2, \\ldots) = \\sigma(\\cup_{j=1}^{\\infty} \\sigma(f_j)) = \\sigma(\\cup_{j=1}^\\infty \\sigma(f_1,\\ldots, f_j)).\\) \\(\\sigma(\\lim \\sup_n f_n) \\subset \\cap_{n=1}^\\infty \\sigma(f_n, f_{n+1},\\ldots).\\) Solution (Ex 19). TODO Exercise 4.6 (Ex 24) Let \\(f\\) be an integrable function on \\((\\Omega, \\F, \\nu)\\). Show that for each \\(\\epsilon &gt; 0\\), there exists a \\(\\delta_\\epsilon\\) such that for \\(A \\in \\F\\): \\[ \\nu(A) &lt; \\delta_\\epsilon \\to \\int_A |f| d\\nu &lt; \\epsilon. \\] Solution. Let \\(\\epsilon &gt; 0\\), \\(A \\in \\F\\) with \\(\\nu(A) &lt; \\delta_\\epsilon = \\frac{\\epsilon}{\\sup_{\\omega \\in A} |f(\\omega)|}\\). Then \\[\\begin{align*} \\int_A |f| d\\nu &amp;\\leq \\int_A \\sup_{\\omega \\in A} |f(\\omega)| d\\nu \\\\ &amp;= \\sup_{\\omega \\in A} |f(\\omega)| \\nu(A) \\\\ &amp;&lt; \\epsilon. \\end{align*}\\] Exercise 4.7 (Ex 30) For any c.d.f. \\(F\\) and any \\(a \\ge 0\\), show that \\(\\int [F(x+a) - F(x)] dx = a\\) Solution. Note that \\(F(X) = \\int 1_{-\\infty, x} dP\\). Then \\[ \\begin{aligned} \\int [F(x+a) - F(x)] dx &amp;= \\int_\\R \\left(\\int 1_{-\\infty, x + a} dP \\right) - \\left(\\int 1_{-\\infty, x} dP \\right) dx\\\\ &amp;= \\int_\\R \\int 1_{x, x + a} dx \\\\ &amp;= a \\end{aligned} \\] Exercise 4.8 (Ex 34) Prove proposition ?? Solution (Ex 34 i)). Let \\(g\\) be the unique function denoted by \\(\\frac{d\\lambda}{d\\nu}\\). Assume \\(f = 1_A\\) for some \\(A\\in \\F\\). Since \\(\\lambda &lt;&lt; \\nu\\), we know that \\(\\lambda(A) = \\int_A g d\\nu\\). So, \\[\\begin{align*} \\int f d\\lambda &amp;= \\int 1_A d\\lambda \\\\ &amp;= \\lambda(A) \\\\ &amp;= \\int_A g d\\nu \\\\ &amp;= \\int 1_A g d\\nu = \\int f g d\\nu. \\end{align*}\\] Hence, (i) is true for all indicator functions, and so by linearity of integrals (??) for all non-negative simple functions. Now, let \\(f\\) be a general non-negative Borel function. Then we know that there exists a sequence of simple functions \\(\\phi_1, \\phi_2, \\ldots\\) such that \\(\\phi_n \\uparrow f\\). Hence, utilizing the monotone convergence theorem and the fact that we know (i) holds for simple functions, \\[\\begin{align*} \\int f d\\lambda &amp;= \\int \\lim_{n \\to \\infty} \\phi_n d\\lambda \\\\ &amp;= \\lim_{n \\to \\infty} \\int \\phi_n d\\lambda \\\\ &amp;= \\lim_{n \\to \\infty} \\int \\phi_n g d\\nu \\\\ &amp;= \\int \\lim_{n \\to \\infty} \\phi_n g d\\nu \\\\ &amp;= \\int f g d\\nu, \\end{align*}\\] and so we have shown that (i) holds for any non-negative Borel function. Solution (Ex 34 ii)). Assume \\(\\lambda_1 &lt;&lt; \\nu\\) and \\(\\lambda_2 &lt;&lt; \\nu\\). Then \\[\\begin{align*} (\\lambda_1 + \\lambda_2)(A) &amp;= \\lambda_1(A) + \\lambda_2(A) \\\\ &amp;= \\int_A g_1 d\\nu + \\int_A g_2 d\\nu \\\\ &amp;= \\int_A (g_1 + g_2)d\\nu, \\end{align*}\\] so \\(\\lambda_1 + \\lambda_2 &lt;&lt; \\nu\\), and \\[\\frac{d(\\lambda_1 + \\lambda_2)}{d\\nu} = g_1 + g_2 = \\frac{d\\lambda_1}{d\\nu} + \\frac{d\\lambda_2}{d\\nu}.\\] Solution (Ex 34 iii)). Since \\(\\tau &lt;&lt; \\lambda\\), \\[\\tau(A) = \\int_A \\frac{d\\tau}{d\\lambda}d\\lambda.\\] Since \\(\\lambda &lt;&lt; \\nu\\), we can use (i) with \\(f = \\frac{d\\tau}{d\\lambda}\\), to get \\[\\tau(A) = \\int_A \\frac{d\\tau}{d\\lambda}\\frac{d\\lambda}{d\\tau} d\\tau,\\] which tells us that \\(\\tau &lt;&lt; \\nu\\) and \\[\\frac{d\\tau}{d\\nu} = \\frac{d\\tau}{d\\lambda}\\frac{d\\lambda}{d\\tau}.\\] Solution (Ex 34 iv)). By definition, \\((\\lambda_1 \\times \\lambda_2)(A) = \\int_A d(\\lambda_1 \\times \\lambda_2) = \\int 1_A d(\\lambda_1 \\times \\lambda_2)\\). Since \\(1_A \\ge 0\\), we can use Fubini to get \\[ (\\lambda_1 \\times \\lambda_2)(A) = \\int \\int 1_A d\\lambda_1 d\\lambda_2. \\] Since \\(\\lambda_1 &lt;&lt; \\nu_1\\), we can use (i) with \\(f = 1_A\\) (\\(1_A\\) is non-negative) to obtain that \\[(\\lambda_1 \\times \\lambda_2)(A) = \\int \\int 1_A \\frac{d\\lambda_1}{d\\nu_1}d\\nu_1 d\\lambda_2,\\] and then, since \\(\\lambda_2 &lt;&lt; \\nu_2\\), using (i) again with \\(f = \\int 1_A \\frac{d\\lambda_1}{d\\nu_1}d\\nu_1\\) (which is non-negative) to get \\[(\\lambda_1 \\times \\lambda_2)(A) = \\int \\int 1_A \\frac{d\\lambda_1}{d\\nu_1}d\\nu_1 \\frac{d\\lambda_2}{d\\nu_2}d\\nu_2.\\] Finally, using Fubini again we get \\[\\begin{aligned} (\\lambda_1 \\times \\lambda_2)(A) &amp;= \\int \\int 1_A \\frac{d\\lambda_1}{d\\nu_1}\\frac{d\\lambda_2}{d\\nu_2} d\\nu_1 d\\nu_2 \\\\ &amp;= \\int_A \\frac{d\\lambda_1}{d\\nu_1}\\frac{d\\lambda_2}{d\\nu_2} d(\\nu_1\\times\\nu_2). \\end{aligned}\\] I.e. \\(\\lambda_1 \\times \\lambda_2 &lt;&lt; \\nu_1 \\times \\nu_2\\) and \\(\\frac{d(\\lambda_1 \\times \\lambda_2)}{d(\\nu_1 \\times \\nu_2)} = \\frac{d\\lambda_1}{d\\nu_1}\\frac{d\\lambda_2}{d\\nu_2}\\). Exercise 4.9 (Ex 35) Let \\(\\{a_n\\}\\) be a sequence of positive numbser with \\(\\sum_{i=1}^\\infty a_n = 1\\), and \\(\\{P_n\\}\\) a sequence of probability measure on a common measurable space, \\((\\Omega, \\F)\\). Define \\(P = \\sum_{n=1}^\\infty P_n\\). Show that \\(P\\) is a probability measure. Let \\(\\nu\\) be a \\(\\sigma\\)-finite measure. Show that \\[P_n &lt;&lt; \\nu \\text{ for all } n \\in \\mb{N}\\quad \\iff \\quad P &lt;&lt; \\nu.\\] Derive the Lebesgue p.d.f. of \\(P\\) when \\(P_n\\) is the gamma distribution \\(\\Gamma(\\alpha, n^{-1})\\) with \\(\\alpha &gt; 1\\) and \\(a_n \\propto n^{-\\alpha}\\). Solution (Ex 35 a)). Need to show that \\(P = \\sum_{n=1}^{\\infty} a_n P_n\\) is a probability measure. So we check the three properties for a probability measure (??), with the extra property that \\(P(\\Omega) = 1\\): \\(P(A) = \\sum_{n=1}^\\infty a_n P_n(A) \\geq 0\\) for all \\(A\\) since \\(a_n &gt; 0\\) for all \\(n\\) by assumption, and \\(P_n(A) \\ge 0\\) for all \\(n\\), since \\(P_n\\) is a probability measure. Furthermore, \\(P(\\Omega) = \\sum_{n=1}^\\infty a_n P_n(\\Omega) = \\sum_{n=1}^\\infty a_n = 1\\), where the second equality holds since \\(P_n\\) is a probability measure (by assumption), and the last equality is exactly the assumption we made about the \\(a_n\\)s. I.e. \\(0 \\le P(A) \\le 1\\). Since \\(P_n(\\emptyset) = 0\\), \\(P(\\emptyset) = \\sum_{n=1}^\\infty a_n P_n(\\emptyset) = 0\\). Let \\(A_1, A_2,\\ldots\\) be a countable sequence of pairwise disjoint sets. Then using that \\(P_n\\) is a measure for all \\(n\\), \\[\\begin{aligned} P(\\cup_{i=1}^\\infty A_i) &amp;= \\sum_{n=1}^\\infty a_nP_n\\left(\\cup_{i=1}^\\infty A_i\\right) \\\\ &amp;= \\sum_{n=1}^\\infty a_n\\sum_{i=1}^\\infty P_n(A_i) \\\\ &amp;= \\sum_{i=1}^\\infty \\sum_{n=1}^\\infty a_n P_n(A_i) \\\\ &amp;= \\sum_{i=1}^\\infty P(A_i). \\end{aligned}\\] Solution (Ex 35 b)). Assume \\(P &lt;&lt; \\nu\\). Let \\(A \\in \\F\\) with \\(\\nu(A) = 0\\). Assume there exists \\(n \\in \\mb{N}\\) such that \\(P_n(A) &gt; 0\\). Then \\(P(A) &gt; a_n P_n(A) &gt; 0\\). But \\(P &lt;&lt; \\nu\\) implies that \\(P(A) = 0\\), so by contradiction, \\(P_n(A) = 0\\) for all \\(n \\in \\mb{N}\\). Now assume \\(P_n &lt;&lt; \\nu\\). Let \\(A \\in \\F\\) with \\(\\nu(A) = 0\\). Then \\(P_n(A) = 0\\) for all \\(n \\in \\mb{N}\\). Hence, \\(P(A) = \\sum_{n=1}^\\infty a_n P_n(A) = 0\\), which means that \\(P &lt;&lt; \\nu\\). Exercise 4.10 (Ex 50) Exercise 4.11 (Ex 55) Let \\(X\\) be a random variable. Show that If \\(E X\\) exists, then \\(E X = \\int_0^\\infty P(X &gt; x) dx - \\int_{-\\infty}^0 P(X \\le x) dx\\). If \\(X\\) has range \\(0, 1, 2, ...\\), then \\(E X = \\sum_{n=1}^\\infty P(X \\ge x)\\) Solution (EX 55 (a)). First rewrite the RHS in terms of PDFS \\[\\int_0^\\infty \\int_x^\\infty f(y) dy dx - \\int_{-\\infty}^0 \\int_{-\\infty}^x f(y) dy dx\\] Then we can use Fubini (since \\(f\\) is positive) to switch the order of the integrals \\[ \\begin{aligned} &amp;= \\int_0^\\infty \\int_0^y f(y) dy dx - \\int_{-\\infty}^0 \\int_{-y}^0 f(y) dy dx \\\\ &amp;= \\int_0^\\infty f(y) \\int_0^y 1 dx dy - \\int_{-\\infty}^0 f(y) \\int_{-y}^0 1 dx dy \\\\ &amp;= \\int_0^\\infty f(y) y dy dx - \\int_{-\\infty}^0 -y \\cdot f(y) dy \\\\ &amp;= E X \\end{aligned} \\] Exercise 4.12 (Ex 56) Calculate the expectation and variance of the noncentral t distribution. Solution (Ex 56 (a)). We did these. The integrals were hard. There were two. We don’t have time to write up the nasty calculus, but do know that we suffered to prove this result. YAY MAAAAATH! We may staple the ugly to the back to the back of the nice solutions. Exercise 4.13 (Ex 65) TODO Exercise 4.14 (Ex 74) Let \\(\\phi_n\\) be a the chf of a probability measure \\(P_n, n = 1, 2, ...\\). Let \\(\\{a_n\\}\\) be a sequence of nonnegative numbers with \\(\\sum_{n=1}^\\infty a_n = 1\\). Show that \\(\\sum_{n=1}^\\infty a_n \\phi_n\\) is a ch.f. and find its corresponding probability measure. Solution (Ex 74). Take \\(P = \\sum_n a_n P_n\\). Note that \\(P_n\\) is dominated by \\(P\\), so that there exists a density \\(g_n = dP_n/dP\\) for each \\(n\\) (exercise 35). Consider \\(P(\\Omega) = \\int_\\Omega \\sum a_n g_n(x) dP = 1\\), so that \\(\\sum a_n g_n(x) = 1\\) almost everywhere. Then \\[ \\begin{aligned} \\sum_n a_n \\phi_n &amp;= \\sum_n a_n \\int e^{itx} g_n(x) dP \\\\ &amp;= \\int \\sum_n a_n e^{itx} g_n(x) dP \\qquad \\text{Fubini} \\\\ &amp;= \\int e^{itx} dP \\\\ &amp;= \\phi(x) \\end{aligned} \\] Exercise 4.15 (Ex 83) Exercise 4.16 (Ex 85) TODO Exercise 4.17 (Ex 93) TODO Exercise 4.18 (Ex 99) LEt \\(X_1, X_2, ...\\) be i.i.d random variables and let \\(Y\\) be a discrete random variable taking positive integer values. Assume that \\(X_i\\) and \\(Y\\) are independent. Obtain the ch.f. of Z Show \\(E Z = E Y E X_1\\) Show Var Z = E Y Var(X_1) + Var(Y) (E X_1)^2 Solution (Ex 99 (a)). TODO Solution (Ex 99 (b)). \\[ \\begin{aligned} E Z &amp;= E(\\sum_{i=1}^Y X_i) \\\\ &amp;= E( E(\\sum_{i=1}^Y X_i) | Y ) \\\\ &amp;= E(Y E(X_1 | Y)) \\\\ &amp;= EY E(E(X_1 | Y)) \\\\ &amp;= EY E X_1 \\end{aligned} \\] Solution (Ex 99 (c)). \\[ \\begin{aligned} Var Z &amp;= Var(E(Z | Y)) + E(Var(Z | Y)) \\\\ &amp;= Var(E(Y X_1 | Y)) + E(Var(Y X_1)) \\\\ &amp;= Var(Y E(X_1 | Y)) + E(Y^2 Var(X_1)) \\\\ &amp;= (E(X_1))^2 Var(Y) + E(Y^2) Var(X_1) \\end{aligned} \\] Exercise 4.19 (Ex 101) TODO Exercise 4.20 (Ex 106) Let \\(\\{Y_n\\}\\) be a sequence of independent random. Show that the following are martingales: \\(X_1 = Y_1\\), \\(X_{n+1} = X_n + Y_{n+1} h_n(X_1, ..., X_n)\\) where \\(h_n\\) are Borel. Suppose \\(E Y_n = 0\\) and \\(Var Y_n = \\sigma^2\\) for all \\(n\\). \\(X_n = (sum_{j=1}^n Y_j)^2 - n \\sigma^2\\) Suppose \\(Y_n &gt; 0\\) and \\(E Y_n = 1\\) for all \\(n\\). \\(X_n = Y_1 \\cdot ... \\cdot Y_n\\). Solution (Ex 106 (a)). Recall that a sequence \\(X_n\\) is a martingale if \\(E(X_{n+1} | X_1, ..., X_n) = X_n\\). \\[ \\begin{aligned} E(X_{n+1} = X_n + Y_{n+1} h_n(X_1, ..., X_n) | X_1, ..., X_n) &amp;= X_n + h_n(X_1, ..., X_n) E(Y_{n+1} | X_1, ..., X_n) \\\\ &amp;= X_n + h_n(X_1, ..., X_n) 0 \\\\ &amp;= X_n \\end{aligned} \\] Solution (Ex 106 (b)). \\[ \\begin{aligned} E(\\sum_{j=1}^{n+1} Y_j)^2 - (n + 1) \\sigma^2 | X_1, ..., X_n) &amp;= E(\\sum_{j=1}^{n} Y_j + Y_{n+1})^2 | X_1, ..., X_n) - (n + 1) \\sigma^2 \\\\ &amp;= E(\\sum_{j=1}^{n} Y_j | X_1, ..., X_n)^2 + E(Y_{n+1} \\cdot \\sum_{j=1}^{n} Y_j | X_1, ..., X_n) + E(Y_{n+1}^2 | X_1, ..., X_n) - (n + 1) \\sigma^2 \\\\ &amp;= E(\\sum_{j=1}^{n} Y_j | X_1, ..., X_n)^2 + E(Y_{n+1}) E(\\sum_{j=1}^{n} Y_j | X_1, ..., X_n) + \\sigma^2 - (n + 1) \\sigma^2 \\\\ &amp;= (\\sum_{j=1}^{n} Y_j)^2 + n \\sigma^2 \\end{aligned} \\] Solution (Ex 106 (c)). \\[ \\begin{aligned} E( Y_1 \\cdot ... \\cdot Y_n \\cdot Y_{n+1} | X_1, ..., X_n) &amp;= Y_1 \\cdot ... \\cdot Y_n E(Y_{n+1} | X_1, ..., X_n) \\\\ &amp;= Y_1 \\cdot ... \\cdot Y_n \\\\ &amp;= X_n \\end{aligned} \\] Exercise 4.21 (Ex 115) Let \\(X_1, X_2, ...\\) be a sequence of identically distributed random variables with finite \\(E |X_1|\\) and let \\(Y_n = n^{-1} \\max_{i \\le n} |X_i|\\). Show that \\(Y_n \\to 0\\) in \\(L_1\\) Show that \\(Y_n \\to 0\\) almost surely Solution (Ex 115 (a)). \\(E Y_n = \\int_0^\\infty {1 \\over n} P(\\max_{i \\le n} |X_i| &gt; t) dt\\) by exercise 1.55. The integrand is then bounded by \\[{1 \\over n} \\sum P(\\max_{i \\le n} |X_i| &gt; t)\\] which equals \\({1 \\over n} P(|X_1| &gt; t)\\) since the \\(X_i\\) are identical. This is finite by hypothesis. Thus we can apply the DCT to see that \\[\\lim E Y_n = \\int_0^\\infty \\lim {1 \\over n} P(\\max_{i \\le n} |X_i| &gt; t) dt = 0\\] Solution (Ex 115 (b)). The condition that \\(E |X_1|\\) converges and being identical gives us that that $ X_n / n$ converges to zero almost surely. then we truncate. for some large enough N, \\(X_n / n\\) must be less than epsilon. then for \\(|X_n| / n\\) for n &lt; N, there are only finitely many cases and so the max exists and is bounded, and goes to zero by the \\(n\\) in the denominator. this gives us that \\(\\lim Y_n\\) goes to zero as \\(n\\) goes to zero. in fact it goes to zero almost everywhere, since the limit \\(|X_n| / n\\) goes to zero almost everywhere by the hypothesis that \\(X_n\\) is absolutely integrable. thus \\(\\lim Y_n\\) goes to zero almost surely. Exercise 4.22 (Ex 117) Exercise 4.23 (Ex 126) Prove (vii) of Theorem 1.8 Solution. Let \\(X_n \\to d X\\), \\(P(X = c) = 1\\). Apply triangle inequality and assumption: \\[ \\lim_{n \\to \\infty} P(\\norm{X_n - c} &gt; \\epsilon) \\le \\lim_{n \\to \\infty} P(\\norm{X_n - X} &gt; \\epsilon) + \\lim_{n \\to \\infty} P(\\norm{X - c} &gt; \\epsilon) = 0. \\] Exercise 4.24 (Ex 127) (a) Suppose X_n converge in distribution to X. Show X_n is O_P(1). Solution (Ex 127 (a)). Suppose X_n converge in distribution to X. For large enough M, we have \\(P(|X| &gt; M) &lt; \\epsilon_1\\) (i.e the tail probability is small). For large enough \\(n\\), $P(|X_n| &gt; _1) - P(|X| &gt; ) &lt; ) &lt; \\(\\epsilon_2\\). Pick \\(n\\) large enough so that \\(\\epsilon_2\\) is controlled. http://users.ices.utexas.edu/~alen/articles/asymp-final.pdf - page 7 Exercise 4.25 (Ex 128) Exercise 4.26 (Ex 137) Let \\(\\{X_n\\}\\) and \\(\\{Y_n\\}\\) be two sequences of R.Vs. Assume \\(X_n \\to_d X\\) and \\(P_{Y_n | X_n = x_n} \\to_w P_Y\\) almost surely for every sequence of numbers \\(\\{x_n\\}\\), where \\(X\\) and \\(Y\\) are independent random variables. Show that \\(X_n + Y_n \\to_d X + Y\\). Solution. \\[ P_{Y_n,X_n} = P_{Y_n|X_n}\\cdot P_{X_n} \\to P_Y P_X = P_{Y,X}, \\] where the last equality holds due to independence of \\(X\\) and \\(Y\\). Exercise 4.27 (Ex 138) Exercise 4.28 (Ex 140) \\(X_n \\sim N(\\mu_n, \\sigma^2_n, n \\in \\N\\) and \\(X \\sim N(\\mu, \\sigma^2)\\). Show that \\(X_n \\to_d X \\iff \\mu_n \\to \\mu\\) and \\(\\sigma_n \\to \\sigma\\). Solution. Assume \\(\\mu_n \\to \\mu\\) and \\(\\sigma_n \\to \\sigma\\). Then \\(f_n(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_n}e^{\\tfrac{-(x-\\mu_n)^2}{\\sigma_n}} \\to f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{\\tfrac{-(x-\\mu)^2}{\\sigma}}\\) for all \\(x\\). Hence, \\(X_n \\to_d X\\). Assume \\(X_n \\to_d X\\), and assume for contradiciton that \\((\\mu_n, \\sigma^2_n) \\to (a,b^2) \\neq (\\mu, \\sigma^2)\\). This implies that \\(f_n = \\frac{1}{\\sqrt{2\\pi}\\sigma_n}e^{\\tfrac{-(x-\\mu_n)^2}{\\sigma_n}} \\to f = \\frac{1}{\\sqrt{2\\pi}b}e^{\\tfrac{-(x-a)^2}{b}}\\), hence \\(X_n \\to_d Y\\) where \\(Y \\sim N(a,b^2)\\). Since \\((a,b) \\neq (\\mu, \\sigma^2)\\), and we know the limiting distribution is unique, this contradicts our assumption. Exercise 4.29 (Ex 142) \\(f_n\\) is the Lebesgue p.d.f. of the t-distribution \\(t_n\\). Show that \\(f_n(x) \\to f(x)\\) for all \\(x \\in \\R\\), where \\(f\\) is the Lebesgue p.d.f. for standard normal. Solution. By definition, \\(f_n(x) = \\frac{\\Gamma(\\tfrac{n+1}{2})}{\\sqrt{n\\pi} \\Gamma(\\tfrac{n}{2})}\\left(1 + \\frac{x^2}{n}\\right)^{\\tfrac{-(n+1)}{2}}\\). Note that \\(\\lim_{n \\to \\infty}\\left(1+\\frac{x}{n}\\right)^n = e^x\\). So, since \\(\\sqrt{1+\\tfrac{x^2}{n}} \\to 0\\), \\[\\left(1 + \\frac{x^2}{n}\\right)^{\\tfrac{-(n+1)}{2}} = \\frac{1}{\\left(1+\\tfrac{x^2/2}{n/2}\\right)^{-n/2}\\sqrt{1+\\tfrac{x^2}{n}}} \\to e^{-x^2/2}.\\] Since \\(\\lim_{n \\to \\infty} \\frac{\\Gamma(n + c)}{\\Gamma(n)n^c} = 1\\), \\[\\lim_{n \\to \\infty} \\frac{\\Gamma(\\tfrac{n}{2} + \\tfrac{1}{2})}{\\Gamma(\\tfrac{n}{2})\\sqrt{n/2}} = 1.\\] So \\[\\begin{aligned} \\lim_{n \\to \\infty} f_n(x) &amp;= \\lim_{n\\to \\infty} \\frac{\\Gamma(\\tfrac{n+1}{2})}{\\sqrt{n\\pi} \\Gamma(\\tfrac{n}{2})}\\left(1 + \\frac{x^2}{n}\\right)^{\\tfrac{-(n+1)}{2}} \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}}e^{-x^2/2}\\lim_{n\\to \\infty}\\frac{\\Gamma(\\tfrac{n}{2} + \\tfrac{1}{2})}{\\Gamma(\\tfrac{n}{2})\\sqrt{n/2}} \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi}}e^{-x^2/2}. \\end{aligned}\\] Exercise 4.30 (Ex 146) Let \\(U_1, U_2, \\ldots\\) be i.i.d. random variables, \\(U_i \\sim U[0,1]\\). Let \\(Y_n = (\\prod_{i=1}^n U_i)^{-1/n}\\). Show that \\(\\sqrt{n}(Y_n - e) \\to_d N(0,e^2)\\). Solution. Let \\(X_i = -log(U_i)\\). Then \\(EX_1 = \\text{Var}(X_1) = 1\\), and implies \\(U_i = e^{-X_i}\\). So \\[\\begin{aligned} Y_n &amp;= \\left(\\prod_{i=1}^n e^{-X_u} \\right)^{-1/n} \\\\ &amp;= e^{\\tfrac{1}{n}\\sum_{i=1}^n X_i}. \\end{aligned}\\] By the CLT (corollary 1.2; page 69), \\(\\tfrac{1}{\\sqrt{n}}\\sum_{i=1}^n (X_i - EX_1) = \\tfrac{n}{\\sqrt{n}}\\left(\\tfrac{1}{n}\\sum_{i=1}^n X_i - 1\\right) \\to_d N(0, 1)\\). Let \\(g = e^{x}\\). Then \\(g^\\prime(x) = g(x)\\), and \\(U_i = g(X_i)\\). Using the delta method (specifically corollary 1.1; page 61), we get that \\[ \\sqrt{n}\\left(Y_n - e\\right) = \\tfrac{n}{\\sqrt{n}}\\left(g\\left(\\tfrac{1}{n}\\sum_{i=1}^n X_i\\right) - g(1)\\right) \\to_d N(0,g(1)^2) = N(0, e^2). \\] Exercise 4.31 (Ex 149) Let \\(X_1, \\ldots X_n\\) be i.i.d. random variables such that for \\(x = 3,4,\\ldots\\), \\(P(X_1 = \\pm x) = (2cx^2 \\log(x))^{-1}\\), where \\(c = \\sum_{x=3}^\\infty \\tfrac{x^{-2}}{\\log(x)}\\). Show that \\(E|X_1| = \\infty\\), but \\(\\tfrac{1}{n} \\sum_{i=1}^n X_i \\to_p 0\\), using Theorem 1.13(i). Solution. Notice that \\[ E|X_1| = \\sum_{x=3}^\\infty 2\\frac{x}{2cx^2\\log(x)} = \\frac{1}{c}\\sum_{x=3}^\\infty \\frac{1}{x\\log(x)} \\ge \\tfrac{1}{c} \\int_3^\\infty \\frac{1}{x\\log(x)}dx = \\infty, \\] and that \\(EX_1 = 0\\). (To see that the inequality above holds, draw it!) Consider \\(nP(|X_1| &gt; n)\\): \\[ \\begin{aligned} nP(|X_1| &gt; n) &amp;= n\\sum(x=n)^\\infty \\frac{1}{2cx^2\\log(x)} \\\\ &amp;\\le \\frac{n}{c} \\int_n^\\infty \\frac{1}{x^2\\log(x)} dx \\\\ &amp;\\le \\frac{n}{c\\log(n)} \\int_n^\\infty \\frac{1}{x^2} dx \\\\ &amp;= \\frac{n}{c\\log(n)} \\frac{1}{n} \\to 0 \\text{ as } n \\to \\infty. \\end{aligned} \\] So by theorem 1.13(i), we \\(\\frac{1}{n}\\sum_{i=1}^n X_i - a_n \\to_p 0\\), where \\(a_n = E(X_1 I_{\\left\\{|X_1| \\le n\\right\\}}) \\to 0\\) (can be seen using an argument as above). Exercise 4.32 (Ex 152) Let \\(T_n = \\sum_{i=1}^n X_i\\), where \\(X_1,X_2,\\ldots\\) are independent and \\(P(X_n = \\pm n^\\theta) = 0.5\\) for some \\(\\theta &gt; 0\\). Show that when \\(\\theta &lt; 0.5\\), then \\(T_n/n \\to_{a.s.} 0\\) Show that when \\(\\theta \\ge 1\\), then \\(T_n/n \\to_{p} 0\\) does NOT hold Solution. (a) Since \\(\\theta &lt; 0.5\\), \\(2(\\theta - 1) &lt; -1\\). So \\[ \\begin{aligned} \\sum_{n=1}^\\infty \\frac{E|X_n|^2}{n^2} &amp;= \\sum_{n=1}^\\infty \\frac{(n^\\theta)^2}{n^2} \\\\ &amp;= \\sum_{n=1}^\\infty n^{2(\\theta - 1)} &lt; \\infty. \\end{aligned} \\] By SLLN, \\(T_n/n = \\frac{1}{n} \\sum_{i=1}^\\infty X_i \\to_{a.s.} EX_1 = 0\\). Note that the p.d.f. of \\(X_n\\) is \\(F_n(x) = 0 1_{X_n &lt; -n^\\theta} + \\tfrac{1}{2}1_{-n^\\theta \\le X_n &lt; n^\\theta} + 1_{X_n \\ge n^\\theta}\\). By definition, \\(X_n \\to_d 0\\) if and only if \\(F_n(x) \\to F(x) = 1_{(X \\ge 0)}\\) for all \\(x\\) continuity points of \\(F\\). For any \\(n &gt; 1\\), \\(F_n(\\tfrac{-n^\\theta}{2}) = 0.5 \\neq F(\\tfrac{-n^\\theta}{2}) = 0\\). So \\(X_n\\) does not converge to \\(0\\) in distribution. By theorem 1.8(iii), this means that \\(X_n\\) does not converge to \\(0\\) in probability (since convergence in probability implies convergence in distribution). Exercise 4.33 (Ex 153) Let \\(X_2, X_3, \\ldots\\) be independent random variables with \\(P(X_n = \\pm \\sqrt{n/\\log(n)}) = 0.5\\). Show that \\(\\sum_{k=1}^\\infty \\frac{E|X_k|^p}{k^p} = \\infty\\) for all \\(p \\in [1,2]\\), but \\(\\lim_{n \\to \\infty}\\frac{1}{n^2} \\sum_{i = 1}^\\infty E|X_i|^2 = 0\\). Solution. For any \\(p\\in [1,2]\\): \\[ \\sum_{n=2}^\\infty \\frac{E|X_n|^p}{n^p} = \\sum_{n=2}^\\infty \\frac{(n/\\log(n))^{p/2}}{n^p} = \\sum_{n=2}^/infty \\frac{1}{(n\\log(n))^{p/2}} = \\infty. \\] Use that \\(\\log(x)\\) is an increasing function: \\[\\begin{aligned} \\lim_{n \\to \\infty} \\frac{1}{n^2} \\sum_{k=2}^n E|X_k|^2 &amp;= \\lim_{n \\to \\infty} \\frac{1}{n^2} \\sum_{k=2}^\\infty \\frac{k}{\\log(k)} \\\\ &amp;\\leq \\lim_{n \\to \\infty} \\frac{(n-1)\\tfrac{n}{\\log(n)}}{n^2} \\\\ &amp;\\leq \\lim_{n \\to \\infty} \\frac{n \\tfrac{n}{\\log(n)}}{n^2} \\\\ &amp;\\leq \\lim_{n \\to \\infty} \\frac{1}{\\log(n)} = 0. \\end{aligned}\\] Exercise 4.34 (Ex 163) Exercise 4.35 (Ex 164) Let \\(X_1, X_2, \\ldots\\) be independent variables with \\(P(X_j = \\pm j^a) = P(X_j = 0) = \\tfrac{1}{3}\\), where \\(a &gt; 0\\), \\(j=1,2,\\ldots\\). Does Liapounov’s condition hold? Solution. \\(\\sigma_n^ = \\sqrt{\\sum_{j=1}^n \\frac{2j^{2a}}{3}}\\). So, want to see if \\[ \\frac{\\sum_{j=1}^n E|X_j - EX_j|^{2+\\delta}}{\\sigma_n^{2+\\delta}} \\to 0. \\] So, \\[\\begin{aligned} \\frac{\\sum_{j=1}^n \\left(\\tfrac{2}{3}j^{2a}\\right)^{2+\\delta}}{\\left(\\tfrac{2}{3}\\sum_{j=1}^n j^{2a}\\right)^{(2+\\delta)/2}} &amp;= \\frac{(\\tfrac{2}{3})^{2+\\delta}}{(\\tfrac{2}{3})^{(2+\\delta)/2}} \\frac{\\sum_{j=1}^\\infty j^{(2+\\delta)a}}{\\left(\\sum_{j=1}^\\infty j^{2a}\\right)^{(2+\\delta)/2}}. \\end{aligned}\\] Choose \\(\\delta = 2\\). Using Jensens inequality with \\(\\phi(x) = x^2\\) and the ratio test, we see that \\[ \\frac{(\\tfrac{2}{3})^4}{(\\tfrac{2}{3})^2} \\frac{\\sum_{j=1}^\\infty ((j^a)^2)^2}{\\left(\\sum_{j=1}^\\infty j^{2a}\\right)^{2}} \\to 0. \\] 4.1.2 Suggested Problems Exercise 4.36 (Ex 3) Let \\(\\Omega, \\F_j)\\), \\(j=1,2,\\ldots\\), be measurable spaces such that \\(\\F_j \\subset \\F_{j+1}\\). Is \\(\\cup_j \\F_j\\) a \\(\\sigma\\)-field? Solution (Ex 3). No. Let \\(\\Omega = [0,1]\\), and \\(\\F_n = \\sigma \\left\\{[0,\\tfrac{1}{2^n}),[\\tfrac{1}{2^n},\\tfrac{2}{2^n}), \\ldots, [\\tfrac{2^{n-1}}{2^n}, 1)\\right\\}\\). Now, consider the set \\(B_n = [0, \\tfrac{1}{2^n})\\). Clearly \\(B_n \\in \\F_n\\) for all \\(n\\), hence \\(B_n \\in \\cup_{i=1}^\\infty \\F_i\\). Hower, \\(\\cap_{i = 1}^{\\infty} B_i = \\{0\\} \\notin \\cup_{i=1}^\\infty \\F_i\\). So \\(\\cup_{i=1}^\\infty \\F_i\\) is not closed under countable intersection, i.e. it is not a \\(\\sigma\\)-algebra. Exercise 4.37 (Ex 6) Prove part (ii) and (iii) of proposition ??. Solution (Ex 6). i) Let \\(A \\subset B\\). Then \\(B \\setminus A \\cap A = \\emptyset\\), hence \\(\\nu(B) = \\nu((B\\setminus A) \\cup A) = \\nu(B\\setminus A) + \\nu(A) \\geq \\nu(A)\\). Let \\(A_1, A_2, \\ldots\\) be a sequence of sets. Define \\(B_i = A_i \\setminus \\left(\\cup_{k = 1}^{i-1} A_k \\right)\\). Then the \\(B_i\\)s are pairwise disjoint. Hence, \\[\\begin{align*} \\nu\\left(\\cup_{i = 1}^\\infty A_i\\right) &amp;= \\nu\\left(\\cup_{i=1}^\\infty B_i\\right) \\\\ &amp;= \\sum_{i=1}^\\infty \\nu(B_i) \\\\ &amp;= \\sum_{i=1}^\\infty \\nu\\left(A_i \\setminus \\left(\\cup_{k=1}^{i-1} A_k \\right ) \\right ). \\end{align*}\\] Since \\(A_i \\setminus \\left(\\cup_{k=1}^{i-1} A_k \\right) \\subset A_i\\), we use (i) to get the result: \\[\\begin{align*} \\sum_{i=1}^\\infty \\nu\\left(A_i \\setminus \\left(\\cup_{k=1}^{i-1} A_k \\right ) \\right ) \\leq \\sum_{i=1}^\\infty \\nu\\left(A_i \\right ) \\end{align*}\\] Let \\(A_1 \\subset A_2 \\subset A_3 \\subset \\dots\\). Define \\(B_i = A_i \\setminus A_{i-1}\\). Then \\(B_1, B_2, \\ldots\\) is a sequence of pairwise disjoint sets, and \\(\\cup_{i = 1}^\\infty B_i = \\cup_{i=1}^\\infty A_i\\). Hence, \\[\\begin{align*} \\nu\\left(\\cup_{i=1}^k A_i\\right) &amp;= \\nu\\left(\\cup_{i=1}^k B_i\\right) \\\\ &amp;= \\sum_{i=1}^k \\nu(B_i) \\\\ &amp;= \\sum_{i=1}^k \\nu(A_i\\setminus A_{i-1}) \\\\ &amp;= \\sum_{i=1}^k \\nu(A_i) - \\nu(A_{i-1}) \\\\ &amp;= \\nu(A_k). \\end{align*}\\] Taking the limit on both sides gives us \\[ \\nu\\left(\\cup_{i=1}^k A_i \\right) = \\lim_{n \\to \\infty} \\nu(A_n). \\] Exercise 4.38 (Ex 15) Show that a monotone function from \\(\\R\\) to \\(\\R\\) is Borel, and a c.d.f. on \\(\\R^k\\) is Borel. Exercise 4.39 (Ex 17) Let \\(f\\) be a non-negative Borel function on \\((\\Omega, \\F)\\). Show that \\(f\\) is the limit of a sequence of simple functions \\(\\{\\phi_n \\}\\) on \\((\\Omega, \\F)\\) with \\(0 \\le \\phi_1 \\le \\phi_2 \\le \\dots \\le f\\). Exercise 4.40 (Ex 23) Exercise 4.41 (Ex 25) Exercise 4.42 (Ex 31) Exercise 4.43 (Ex 36) Exercise 4.44 (Ex 46) Exercise 4.45 (Ex 53) Exercise 4.46 (Ex 57) Exercise 4.47 (Ex 61) Exercise 4.48 (Ex 66) Exercise 4.49 (Ex 70) Exercise 4.50 (Ex 73) Exercise 4.51 (Ex 78) Exercise 4.52 (Ex 79) Exercise 4.53 (Ex 81) Exercise 4.54 (Ex 82) Exercise 4.55 (Ex 86) Exercise 4.56 (Ex 88) Exercise 4.57 (Ex 91) Exercise 4.58 (Ex 97) Exercise 4.59 (Ex 98) Exercise 4.60 (Ex 102) Exercise 4.61 (Ex 104) Exercise 4.62 (Ex 114) Exercise 4.63 (Ex 116) Exercise 4.64 (Ex 116) Exercise 4.65 (Ex 118) Exercise 4.66 (Ex 119) Exercise 4.67 (Ex 121) Exercise 4.68 (Ex 122) TODO - Alex Exercise 4.69 (Ex 125) Exercise 4.70 (Ex 136) Exercise 4.71 (Ex 141) Exercise 4.72 (Ex 144) Exercise 4.73 (Ex 145) Exercise 4.74 (Ex 150) Exercise 4.75 (Ex 154) Exercise 4.76 (Ex 156) Exercise 4.77 (Ex 161) Exercise 4.78 (Ex 166) Note: This could be done in one step, but I found it easier to split up into two.↩ "]
]
